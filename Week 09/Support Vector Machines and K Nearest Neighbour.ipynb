{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
<<<<<<< HEAD
    "## Notes on distance measures and basis functions"
=======
    "## Notes on distance measures and basis functions\n",
    "We need to just review a few notes on distances measures, that we will need during the next few slides."
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Euclidian distance in D dimensions is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$d( {\\bf x_i},{\\bf x_k} ) \\equiv \\sqrt{\\left(x^{1}_{i} - x^{1}_{k}\\right)^2 + \\left(x^{2}_{i} - x^{2}_{k}\\right)^2 + \\ldots + \\left(x^{D}_{i} - x^{D}_{k}\\right)^2} = \\sqrt{\\sum_{j=1}^D \\left( x^{j}_{i} - x^{j}_{k} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The cosine similarity measure in D dimensions is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$s({\\bf x_i},{\\bf x_k}) \\equiv cos(\\angle ({\\bf x_i},{\\bf x_k})) = \\frac{\\sum_{j=1}^{D} x_{i}^{j} x_{k}^{j}}{\\sqrt{   \\sum_{j=1}^{D} \\left( x_{i}^{j} \\right)^2} \\sqrt{\\sum_{j=1}^{D} \\left( x_{k}^{j} \\right)^2 }} = \\ldots ??$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The standard Radial Basis Function (RBF) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ k({\\bf x},{\\bf x'}) \\equiv \\textrm{exp} \\left( - \\frac{\\| {\\bf x} -{\\bf x'} \\|^2}{2\\sigma^2} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "metadata": {},
   "source": [
    "# Where Are We At....?\n",
    "![MLPaths](images/MLPaths.png)"
   ]
  },
  {
   "cell_type": "markdown",
>>>>>>> upstream/master
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear SVM Classification -- 'Hard Margin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example of Determining Decision Boundary\n",
    "The simplest two-class example we can come up with... is easily separable.\n",
    "Fit function $ g(\\vec x) = \\vec w \\vec x + w_0 $ to the points, where $g(1,1) = -1$ and $g(2,3) = 1$.\n",
    "Weight vector, for simple case, we know, is $(2,3) - (1,1) = \\vec w = a(1,2) = (a,2a)$.\n",
    "Two equations in two unknowns:\n",
    "$a + 2a + w_0 = -1$, and\n",
    "$2a + 6a + w_0 = +1$, yielding $a=\\frac{2}{5}$ and $w_0 = -\\frac{11}{5}$.\n",
    "Finally, our decision boundary is given by: $$g(\\vec x) = \\frac{2}{5}x_1 + \\frac{4}{5}x_2 - \\frac{11}{5}.$$\n",
    "Our **support vector(s)** is $\\vec w = \\left(\\frac{2}{5},\\frac{4}{5}\\right)$. We only have one (why?). And it has only two components (why?).\n",
    "\n",
    "![LinearSVM](images/LinearSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Let's Go To \"Hands-On\" Code Repo Resources, Chapter 5 \n",
<<<<<<< HEAD
    "http://localhost:8888/notebooks/soft2019spring/AI/Week%2009/resources/LlinearSVM.ipynb\n"
=======
    "https://github.com/datsoftlyngby/soft2019spring-ai/blob/master/Week%2009/resources/LlinearSVM.ipynb"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear SVM Classification -- 'Soft Margin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What happened here?** Now there is \"noise\" in training data. We have two choices:\n",
    "  1. Find a new decision boundary that respects the new data point.\n",
    "    - not the right one we argue, but can you tell us why??\n",
    "  2. Make our current procedure \"sloppy\".\n",
    "    - better choice, when considering just this single rogue data point.\n",
    "![LinearRgularizedSVM](images/LinearRegularizedSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to deal with the stray data point cases, so that we do not overfit and lose valuable classification generalizing power.\n",
    "\n",
    "We introduce a hyperparameter to soften the blow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The \"Soft Margin\" Classifier -- One (only so far....) Hyperparameter (regularization)\n",
    "From 'Hard Margin' case we have:\n",
    "1. ${\\bf w} {\\bf x}_i - b \\geq 1$, if $y_i = + 1$ and \n",
    "2. ${\\bf w} {\\bf x}_i - b \\geq 1$, if $y_i = - 1$.\n",
    "\n",
    "\n",
    "Also; minimize $\\|{\\bf w}\\|$ (or eq. $\\frac{1}{2}\\|{\\bf w}\\|^2$), so $y_i({\\bf w} {\\bf x}_i - b) - 1 \\geq 0$ to get maximum separation to support vectors.\n",
    "\n",
    "**This does not work for \"noisy\" data**. We need to do something.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## The Regularization Parameter, _C_\n",
    "Introduce the 'Hinge' function to the loss, $\\textrm{max}(0, 1 - y_i({w}{x}_i-b)$, which is zero if 1 and 2 above is OK. But if point on wrong side of decision boundary, 'hinge' is $\\propto$ distance from D.B.\n",
    "\n",
    "The total cost (to minimize) becomes\n",
    "$$C \\|{\\bf w}\\|^2 + \\frac{1}{N}\\sum_{i=1}^{N} \\textrm{max}(0, 1 - y_i({w}{x}_i-b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Let's Go To \"Hands-On\" Code Repo Resources, Chapter 5 \n",
    "Here we briefly explore the idea of the soft margin (or regularization hyperpxarameter).\n",
    "Many other types of hyperparameters come into play later, as we dive into non-linear SVMs, but for now let's stay low:\n",
<<<<<<< HEAD
    "http://localhost:8888/notebooks/soft2019spring/AI/Week%2009/resources/SoftMargin.ipynb"
=======
    "https://github.com/datsoftlyngby/soft2019spring-ai/blob/master/Week%2009/resources/SoftMargin.ipynb"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-Linear SVM Classification, the 'kernel trick', hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data are not linearly separable in feature space\n",
    "When this happens we need to create different, often complex, decision boundaries (hypersurfaces), other than a simple line (in 2D feature space). \n",
    "\n",
    "The process of introducing a new dimension (or more) leads to separability:\n",
    "![NonlinearData1](images/nonlinear/Slide1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data are not linearly separable in feature space\n",
    "When this happens we need to create different, often complex, decision boundaries (hypersurfaces), other than a simple line (in 2D feature space). \n",
    "\n",
    "The process of introducing a new dimension (or more) leads to separability:\n",
    "![NonlinearData3](images/nonlinear/Slide3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data are not linearly separable in feature space\n",
    "When this happens we need to create different, often complex, decision boundaries (hypersurfaces), other than a simple line (in 2D feature space). \n",
    "\n",
    "The process of introducing a new dimension (or more) leads to separability:\n",
    "![NonlinearData2](images/nonlinear/Slide2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Let's Go To \"Hands-On\" Code Repo Resources, Chapter 5 \n",
    "Demonstration of non-linearly separable data, the kernel trick, and a few hyperparameters.\n",
<<<<<<< HEAD
    "http://localhost:8888/notebooks/soft2019spring/AI/Week%2009/resources/NonLinearSVM.ipynb"
=======
    "https://github.com/datsoftlyngby/soft2019spring-ai/blob/master/Week%2009/resources/NonLinearSVM.ipynb"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
>>>>>>> upstream/master
   "source": [
    "## Summing up on SVMs\n",
    "There are many, many more aspects of SVMs that we won't have time to cover. But take a look -- if you \"hungry\" -- at the Skikit-Learn tutorials' libraries. Here you'll find a nicely segmented library, specifically for SVMs, here: \n",
    "https://scikit-learn.org/stable/auto_examples/index.html#support-vector-machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
<<<<<<< HEAD
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scikit-Learn has some great demonstrations of the KNN algorithm. Let's go there for a hand-on tutorial...\n",
    "\n",
    "They look again at the Iris flower classification data, so we're familiar with the data content at least.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
=======
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KNN as a Classifier\n",
    "Let's check the KNN on a set of points, and talk about things _qualitatively_.\n",
    "\n",
    "![KNNTest](images/knn/KNN_test_data.png)"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
=======
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scikit-Learn has some great demonstrations of the KNN algorithm. Let's go there for a hand-on tutorial. They look again at the Iris flower classification data, so we're familiar with the data content at least: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
   ]
>>>>>>> upstream/master
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.7"
=======
   "version": "3.7.1"
>>>>>>> upstream/master
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
