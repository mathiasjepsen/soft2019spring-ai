{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "* Derivatives\n",
    "* Gradients and partial derivatives\n",
    "* Gradient descents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear models\n",
    "\n",
    "$f(x) = \\alpha x + \\beta$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1.5 * x + 10\n",
    "\n",
    "xs = np.arange(0, 100)\n",
    "plt.plot(xs, f(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylabel('Murders in million people')\n",
    "plt.xlabel('Growth rate in %')\n",
    "plt.plot(xs, f(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How many murders do we get if our growth rate is 2%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylabel('Murders in million people')\n",
    "plt.xlabel('Growth rate in %')\n",
    "plt.plot(xs, f(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What is the minimum amount of murders we can get, if we have a positive inflation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-linear models\n",
    "\n",
    "\n",
    "\\begin{align} y =\\tfrac{1}{4}x^3+\\tfrac{3}{4}x^2-\\tfrac{3}{2}x + 10 \\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return x ** 3 / 4 + 3 * x ** 2 / 4 - 3 * x / 2 + 10\n",
    "\n",
    "plt.ylabel('Murders')\n",
    "plt.xlabel('Growth rate')\n",
    "\n",
    "xs = np.arange(-5, 5, 0.1)\n",
    "plt.plot(xs, g(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How many murders do we get if our growth rate is 2%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xs, g(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What is the minimum amount of murders we can get, if we have a positive inflation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimum\n",
    "\n",
    "* Global: The best value for the **entire** model\n",
    "* Local: The best value for the model, in a local, small place\n",
    "\n",
    "\n",
    "* **Very** important term\n",
    "  * Models exist to be optimised: deaths, cancer, stock prices..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss functions and optimum\n",
    "\n",
    "* Your models has a loss function\n",
    "  * Remember the MAE or RMSE\n",
    "* You want to keep the loss function *low*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Think about yourself as a human. You want to minimise pain and optimise pleasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* New life purpose: How do we find the local or global optimum?\n",
    "  * Video: [CCC: The ghost in the machine](https://media.ccc.de/v/35c3-10030-the_ghost_in_the_machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Function gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align} y =\\tfrac{1}{4}x^3+\\tfrac{3}{4}x^2-\\tfrac{3}{2}x + 10 \\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\\begin{align} y =\\tfrac{3}{4}x^2+\\tfrac{6}{4}x-\\tfrac{3}{2} \\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def g_prime(x):\n",
    "    return 3 / 4 * x ** 2 + 6 / 4 * x - 3 / 2\n",
    "\n",
    "plt.plot(xs, g(xs))\n",
    "plt.plot(xs, g_prime(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xs, g(xs))\n",
    "plt.plot(xs, g_prime(xs))\n",
    "plt.plot(np.arange(-5, 5), np.repeat(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(np.arange(-5, 5), np.arange(-5, 5))\n",
    "plt.gca(projection='3d') # plt.gca: Get Current Axis\n",
    "plt.gca().plot_surface(X, Y, X ** 2 + Y ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimating gradients\n",
    "\n",
    "* Goal: figure out how $f(x)$ changes when $x$ changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, delta):\n",
    "    return (f(x + delta) - f(x)) / delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xs, f(xs))\n",
    "plt.plot(xs, difference_quotient(f, xs, 0.001), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "difference_quotient(f, 0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot actual and estimated derivative of f(x) = x^2\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "def derivative_estimate(x):\n",
    "    return difference_quotient(square, x, delta=0.00001)\n",
    "\n",
    "xs = np.arange(-10,10)\n",
    "plt.plot(xs, square(xs))\n",
    "plt.plot(xs, derivative(xs), 'rx')           # red  x\n",
    "plt.plot(xs, derivative_estimate(xs), 'b+')  # blue +\n",
    "plt.show() # purple *, hopefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differentiation\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "$f'(x) = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We are looking for the change $d$ in $f(x)$, when we change $x$ with $d$\n",
    "  * Remember: `(f(x + delta) - f(x)) / delta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "${df \\over dx} = f'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear models\n",
    "\n",
    "$\\alpha x_1 + \\beta x_2 + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... But how do we take the derivative of $x_1$, **and** $x_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Answer: By deriving one variable at the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradients and partial derivatives\n",
    "\n",
    "$f(x, y) = x^2 + xy + y^2 = z$\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/2d/Partial_func_eg.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Derivative between $x$ and $z$\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/fe/X2%2BX%2B1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can say that our function $f(x, y) = x^2 + xy + y^2$ is the same as $f_y(x) = x^2 + xy + y^2$\n",
    "\n",
    "In Python this is the same as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fy(y):\n",
    "    def fx(x): \n",
    "        return x ** 2 + x * y + y ** 2\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fy(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So now we know that $y$ is constant, so that means that the derivative of \n",
    "\n",
    "$f_y(x) = x^2 + xy + y^2$ \n",
    "is\n",
    "\n",
    "$f_y'(x) = 2x + y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In math, this is written\n",
    "\n",
    "${\\partial f \\over \\partial x}(x, y) = 2x + y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Partial derivative\n",
    "\n",
    "$\\frac{df_{a_1,\\ldots,a_{i-1},a_{i+1},\\ldots,a_n}}{dx_i}(a_i) = \\frac{\\partial f}{\\partial x_i}(a_1,\\ldots,a_n).$\n",
    "\n",
    "Taking the derivative of a function with many inputs, does not just give you *one* derivative, but *a list* of **partial derivatives**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradients\n",
    "\n",
    "$\\nabla f(a) = \\left(\\frac{\\partial f}{\\partial x_1}(a), \\ldots, \\frac{\\partial f}{\\partial x_n}(a)\\right)$\n",
    "\n",
    "In one point $a$, this gives us a list of partial derivatives, that tells us which direction each dimension is moving, in that exact point $a$.\n",
    "\n",
    "This is called a **gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Gradient descent\n",
    "\n",
    "Forest analogy: You're lost in the mountain and has to find your way down\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/c/c7/Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Math analogy: You have a function, and the only thing you know is the *direction* it moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In multiple dimensions the *direction* is in multiple dimensions. So we don't just need the derivative of the function in one dimension, we need it in multiple dimensions! \n",
    "\n",
    "We need the gradients! That's why it's called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent: summary\n",
    "\n",
    "* We have a loss function for our models\n",
    "  * We like our loss to be **small**\n",
    "* That loss function can be in multiple dimensions\n",
    "  * It's **hard** to predict where the loss function is small\n",
    "* Gradients gives us an idea on the *direction* the function is going\n",
    "  * Direction **small** is good because it means a small loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient descent steps in the direction of the loss\n",
    "  * Until you find the smallest point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent example with science spending data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"science.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.array(data['US science spending']).reshape(-1, 1)\n",
    "ys = np.array(data['Suicides']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 1, y = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "model.fit(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.predict([[100], [10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 1, y = 2)\n",
    "plt.plot(xs, model.predict(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scaling data\n",
    "\n",
    "Gradient descent goes in the direction of the gradient with respect to x.\n",
    "If that gradient is very large, the steps we take are laaaarge.\n",
    "\n",
    "What can we do to fix that?\n",
    "Scale the data to be smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs_scaled = scale(xs)\n",
    "xs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "model.fit(xs_scaled, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xs_scaled, ys)\n",
    "plt.plot(xs_scaled, model.predict(xs_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why don't we get any good results?!\n",
    "\n",
    "To get the optimal solution, we have to take many steps towards the correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Challenge:\n",
    "\n",
    "* How many steps is the model taking now? (read the documentation)\n",
    "* Can you make it take 100 steps? 1000 steps? 10000 steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "* Derivatives\n",
    "  * The **rate of growth** for a function $f$\n",
    "  * Normally defined at points like $x$: $f'(x)$\n",
    "* Partial derivatives\n",
    "  * Derivatives in multiple dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient\n",
    "  * A vector of derivatives, one for each dimension in a function $f$\n",
    "* Gradient descent\n",
    "  * A way to optimise a function by moving towards an optimum\n",
    "  * For instance minimising a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Scaling data \n",
    "  * Standardising data into a smaller space"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
