{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Nearest Neighbour (kNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Special case is k=1\n",
    "Property of kNN for k=1, is Voronoi tesselation. Each data point _defines_ a region as follows:\n",
    "\n",
    "## $$R_i = \\{ \\vec x : d(\\vec x, \\vec x_i) < d(\\vec x, \\vec x_j), i \\neq j \\}$$\n",
    "\n",
    "Which gives us decision boundaries as so:\n",
    "![Voronoi](images/knn/voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## kNN as a Classifier\n",
    "Let's check the kNN on a set of points, and talk about things _qualitatively_.\n",
    "This data set is a bit nasty... let's also talk about why that is.\n",
    "\n",
    "![KNNTest](images/knn/KNN_test_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Two Classes of kNN\n",
    "1. **k-NN classification**: the output is a **class membership**. An object is classified by a plurality vote of its neighbors.\n",
    "2. **k-NN regression**: the output is the **property value** for the object. This value is the average of the k nearest neighbors' values.\n",
    "\n",
    "## Two types of kNN\n",
    "1. Simple kNN, with equal weighting of the neighbors, and\n",
    "2. Weigthed kNN, where neighbors are weighted according to some prescription, usually something like $w_i \\propto \\frac{1}{dist(\\vec r_i, r_k)}$.\n",
    "\n",
    "## Rules (and rules of thumb) when using kNN\n",
    "1. for binary classification, k _must_ be odd. **_Why is that??_**\n",
    "2. k must _not_ be a multiple of number of classes. **_Why is that??_**\n",
    "3. complexity in neighbor search can be preventive for large data sets.\n",
    "4. for large data sets, either choose something else, or need new algorithm.\n",
    "\n",
    "### Concluding \n",
    "This quick walk-through, has shown us a classifier that is very sensitive to distance, so we should make an important observation when using kNN for classification. Up next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Repeat notes on distance measure\n",
    "We need to just review Euclidian distance, and two formula..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Euclidian distance in D dimensions is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### $$d( {\\bf x_i},{\\bf x_k} ) \\equiv \\sqrt{\\left(x^{1}_{i} - x^{1}_{k}\\right)^2 + \\left(x^{2}_{i} - x^{2}_{k}\\right)^2 + \\ldots + \\left(x^{D}_{i} - x^{D}_{k}\\right)^2} = \\sqrt{\\sum_{j=1}^D \\left( x^{j}_{i} - x^{j}_{k} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Volume of a _unit_ hypercube (length 1 along all edges) in n dimensions is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### $$V_{cube}(n) = 1^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Volume of a _unit_ sphere (radius one in all directions and dimensions) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### $$V_{sphere}(r\\equiv 1, n) = \\frac{\\pi^{\\frac{n}{2}}}{\\Gamma(n+\\frac{1}{2})}1^n = \\frac{\\pi^{\\frac{n}{2}}}{\\Gamma(n+\\frac{1}{2})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which brings us to the notion of **_The Curse of Dimensionality_**: data are more sparsely represented, basically, in higher dimensions. So distance becomes meaningless since all points are statistically about equi-distant in data space!\n",
    "![Curse](images/knn/curse_cats_dogs.png)\n",
    "So to do k-Nearest Neighbors classification, if you have more than about 7-10 features, you should do data reduction first. But how to that is a story you should read when we get there, later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scikit-Learn has some great demonstrations of the KNN algorithm. Let's go there for a hand-on tutorial. They look again at the Iris flower classification data, so we're familiar with the data content at least: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
