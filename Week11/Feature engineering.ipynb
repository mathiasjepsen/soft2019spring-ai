{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A feature vector\n",
    "\n",
    "\"A feature vector is a vector in which each dimension $j = 1, \\dots , D$ contains a value that describes the example somehow.\"\n",
    "\n",
    "* \"A feature vector is a vector in which...\" \n",
    "  * This means that we have an unnamed vector that we're trying to define\n",
    "* \"each dimension $j = 1, \\dots , D$\"\n",
    "  * Now we know we have a variable $j$ and that $j$ contains a number of things\n",
    "  * The thing starts with the 1 and ends with D -- Oh, a second variable!\n",
    "  * What is D? ... Could be 100; let's say it's 100\n",
    "  * So D is the number of dimensions, then D must be the length! And if D is 100 then the unnamed feature vector is 100 elements long\n",
    "* \"contains a value that describes the example\"\n",
    "  * Ok, so each thing inside the unnamed vector (the thing with number $1, \\dots, D$) helps us describe one piece of example data\n",
    "  * Ok, that must mean that element $1$ is describing something different that element $2$\n",
    "  * ... I don't understand much more, let's move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"That value is called a *feature* and is denoted as $x^j$\"\n",
    "\n",
    "* Ok, so now we have a name for our list called $x$\n",
    "* We also heard that each single element (the ones with numbers $1, \\dots, D$) is called a **feature**\n",
    "  * And each element is uniquely identifiable by $j$: $x^j$\n",
    "  * Hey, I've seen that before! That's an index!\n",
    "  * Ahhhh, easy, so this just means that I can look at an element in the vector by it's index!\n",
    "  * ... Oh, and that element is then for some reason called a *feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We have a vector x of 1, ..., D elements\n",
    "D = 100\n",
    "x = np.arange(1, D + 1)\n",
    "\n",
    "# Apparently we can look up a certain element in the list given j:\n",
    "j = 1\n",
    "x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Be careful with indexing!\n",
    "print(x[0])\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **feature vector** is then simply just a list where we can look an element up by its index.\n",
    "\n",
    "Math: $x^1$\n",
    "\n",
    "Python:\n",
    "```python\n",
    "x[0]\n",
    "    ```\n",
    "\n",
    "Math: $x^{100}$\n",
    "\n",
    "Python: \n",
    "```python\n",
    "x[99]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature vectors as descriptions of things\n",
    "\n",
    "Let's say we have a Person class:\n",
    "\n",
    "```python\n",
    "\n",
    "class Person:\n",
    "    \n",
    "    def __init__(self, age, height):\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "```\n",
    "\n",
    "A Person is described by two **features**: `age` and `height`\n",
    "\n",
    "* So a **feature vector** for a person would have 2 dimensions\n",
    "  * $x^1 = $ age\n",
    "  * $x^2 = $ height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature vectors in high dimensions\n",
    "\n",
    "* Imagine an image with the resolution `1024 x 800`\n",
    "  * How many pixels does that image have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* That image has `1024 * 800 = 819200` features!\n",
    "```python\n",
    "x[0]      # First feature\n",
    "x[819199] # Last feature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivating the problem\n",
    "\n",
    "“We need to be able to predict whether a particular\n",
    "customer will stay with us. Here are the logs of customers’ interactions with our product for\n",
    "five years.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What information can we get from a log?\n",
    "\n",
    "* Timestamp\n",
    "* Number of connections\n",
    "* Frequency of connections\n",
    "* Session duration\n",
    "* ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The more precisely we can describe the data, the better our model gets!\n",
    "\n",
    "We want to select the information that is important and *remove* the ones that aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "The problem of transforming raw data into a dataset is called feature engineering.\n",
    "\n",
    "Goal: describe data with *informative* features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Today: techniques to encode data in specific ways\n",
    "\n",
    "* One-hot\n",
    "* Binning\n",
    "* Normalisation\n",
    "* Standardisation\n",
    "* Dealing with missing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Imagine $n$ features. One-hot encoding means setting **one** feature to 1 and the rest to 0.\n",
    "\n",
    "Example:\n",
    " * Features `[Left, Right]`: `[1, 0]`\n",
    " * Features `[M, T, W, T, F, S, S]`: `[0, 0, 1, 0, 0, 0]`\n",
    " * Features `[M, F, X]`: `[0, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "xs = [['Male'], ['Female'], ['Female']]\n",
    "encoder.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "encoder.fit_transform(xs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "xs = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "encoder.fit_transform(xs).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binning\n",
    "\n",
    "Imagine a list of numbers, say ages, that you want to cluster into groups.\n",
    "\n",
    "Binning is to take numerical data and turn it into categories.\n",
    "\n",
    "Examples:\n",
    "* Age to groups: 0-10, 11-20, 21-30, ...\n",
    "* Speed to groups: Slow, Medium, Fast, Ultra mega fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "binner = KBinsDiscretizer(n_bins=10)\n",
    "age = np.random.randint(0, 100, (1000, 1)) # Genrate 1000 random numbers\n",
    "binner.fit_transform(age).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 91.,  97., 109., 102.,  95.,  94., 106., 102.,  94., 110.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner.fit_transform(age).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_discretization_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Discretisation strageties](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py) in KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalisation\n",
    "\n",
    "Normalization is the process of converting an actual range of values which a numerical feature can take, into a standard range of values, typically in the interval\n",
    "$[−1,1]$ or $[0,1]$.\n",
    "\n",
    "$$\\bar{x} = { x - min_x \\over max_x - min_x }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.random.random((1, 20)) * 100\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "Normalizer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardisation\n",
    "Standardization (or z-score normalization) is the procedure during which the feature values are rescaled so that \n",
    "they have the properties of a standard normal distribution with μ = 0 and σ = 1, where μ \n",
    "is the mean (the average value of the feature, averaged over all examples in the dataset) and σ \n",
    "is the standard deviation from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standard deviation ($\\sigma$)\n",
    "\n",
    "Describes the **variance** in data. How big of a spread is there around the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1920px-Standard_deviation_diagram.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Standardisation scales AND standardises values to fit the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "* Removing them\n",
    "  * `pd.dropna`\n",
    "* Extrapolating (imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation techniques\n",
    "\n",
    "* Imagine a dataset where you have a lot of missing values for *one* feature\n",
    "\n",
    "\n",
    "* Imagine a dataset that you cannot hand out (NDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation technique 1: Averaging\n",
    "\n",
    "Replace the missing value with the average value of the feature:\n",
    "\n",
    "$$x = {1 \\over N} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation technique 2: Sampling\n",
    "\n",
    "Imagine that the data is distributed in a certain way (for instance the normal distribution).\n",
    "You can now sample new \"*mock*\" data from that distribution.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_missing_values_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: Danske Bank customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation in sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit_transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
